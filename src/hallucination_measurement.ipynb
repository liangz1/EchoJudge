{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddfcaba-ea1d-45fb-b335-0d1c4dc817e3",
   "metadata": {},
   "source": [
    "# Has GPT-4 or GPT-4-turbo solved the hallucination issue with public Databricks Docs?\n",
    "\n",
    "Can we remove retrieval from Databricks Docs?\n",
    "\n",
    "TLDR: \n",
    "* No. For an Assistant that provides **accurate** and **latest** knowledge in the public Databricks Docs, an ideal context is still useful to correct factual errors from GPT-4-turbo, GPT-4, and GPT-3.5.\n",
    "* Hallucination severity: GPT-3.5 > GPT-4 > GPT-4-turbo\n",
    "* With a bad context, GPT-4-turbo(no context) often gives a better answer.\n",
    "\n",
    "## Evaluation Dataset\n",
    "20 synthetic questions from the [source dataset](https://docs.google.com/spreadsheets/d/1pXpjCFoAfP81m0rZ6Y60Dcfc9Ne3SRwg7W1XCMwjpCg/edit#gid=106621394) which is generated from Databricks Docs website chunks by anirudh.kondaveeti@.\n",
    "\n",
    "Schema:\n",
    "**generated_question**: str, **generated_ground_truth_answer**: str, **ground_truth_chunk_text**: str\n",
    "\n",
    "Collected answers from GPT-3.5, GPT-4 and GPT-4-turbo using [this notebook](https://e2-dogfood.staging.cloud.databricks.com/?o=6051921418418893#notebook/3639518212483601).\n",
    "\n",
    "## Manual Grading criteria\n",
    "Manually graded by liang.zhang@:\n",
    "- -1: The main point contains **key factual errors**.\n",
    "- 0: Plausible answer in general, but **incorrect facts** are found among correct facts.\n",
    "- 1: Correct and easy to read answer, helpful, and **no factual errors** found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f109a2eb-272a-46c0-8a46-5a76c7bb4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "import ast\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import trace_utils\n",
    "import yaml\n",
    "importlib.reload(trace_utils)\n",
    "from trace_utils import Trace, Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f2032e60-ecb4-4a77-83a3-e6dafe996406",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = Traces.load_from_yaml_file(\"../data/databricks_docs_synthetic.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72413689-6d02-4385-87be-a8b33d36d964",
   "metadata": {},
   "source": [
    "## Ground truth answers\n",
    "\n",
    "The score is < 1 sicne there is one trace whose ground truth answer has factual error due to truncated context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bed31196-9e84-4888-b121-7c11ffe166bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"generated_ground_truth_answer\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bea90d-aad4-481d-b226-005c2f4057df",
   "metadata": {},
   "source": [
    "## Ideal context + {GPT-4-turbo, GPT-4, GPT-3.5}\n",
    "\n",
    "With ground truth context, GPT-4-turbo and GPT-4 can generate ideal answers in most cases, and are about the same quality. GPT-3.5 is slightly weaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "665e51f7-3ca6-4cd8-8459-5289ae6bae27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"answered_by_gpt_35_with_ground_truth_context\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cd2d6e55-181b-4bbc-96c6-170d8f917966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"answered_by_gpt_4_with_ground_truth_context\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9441d145-d7b6-4af0-bea0-9ff20f38167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"answered_by_gpt_4_turbo_with_ground_truth_context\", judge_name=\"liang.zhang@\", criterion=\"answer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc8825-9bd8-47f7-a054-bd10bb5a9a6f",
   "metadata": {},
   "source": [
    "## No context + {GPT-4-turbo, GPT-4, GPT-3.5}\n",
    "\n",
    "GPT-4-turbo is much less likely to give terrible wrong (hallucinated) answers, while GPT-3.5 often does so. However, they still struggle with accurate factual details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6e6972d-7dd4-4ea5-a64a-7e0f717c3448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"directly_answered_by_gpt_35\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5da48fb-3e71-442e-98f4-9ce3114c8e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.25"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"directly_answered_by_gpt_4\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "481498c6-de9a-413c-977f-e324ecf18720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"directly_answered_by_gpt_4_turbo\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176186ab-76c7-45a3-a5ea-c5d483441ce7",
   "metadata": {},
   "source": [
    "## What questions can be answered well by GPT-4-turbo + no context?\n",
    "\n",
    "The \"hallucination\" happens to match the facts about Databricks.\n",
    "\n",
    "The questions have the following characteristics:\n",
    "\n",
    "* Facts that exist for a long time (such as schedule a notebook, read binary files, incremental data ingestion).\n",
    "* General knowledge, concepts (such as open Delta Sharing).\n",
    "\n",
    "[Sample answers](https://github.com/liangz1/EchoJudge/issues/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "77e4d170-8db6-4f1d-8c53-a2f71ae03665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a recipient in Unity Catalog?\n",
      "What is open Delta Sharing?\n",
      "What is the recommended method for incremental data ingestion in Databricks?\n",
      "How can you schedule a notebook as a task in Databricks?\n",
      "How do I read binary files using Databricks Runtime?\n"
     ]
    }
   ],
   "source": [
    "ts = traces.select(responder_name=\"directly_answered_by_gpt_4_turbo\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=1)\n",
    "for s in ts.traces:\n",
    "    print(s.user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae17c4f-d372-41c3-90a2-d447f5bd08e8",
   "metadata": {},
   "source": [
    "## What questions are answered poorly by GPT-4-turbo + no context?\n",
    "\n",
    "The \"hallucination\" is plausible but does not match the facts about Databricks.\n",
    "\n",
    "The questions have the following characteristics:\n",
    "\n",
    "* New features, new UI component, new recommended User Journey (such as Auto Loader vs COPY INTO; Databricks Lakehouse Monitoring, AI functions; schedule query execution).\n",
    "* Knowledge that involves subtle nuonces (such as UDAF vs UDF in Spark SQL).\n",
    "\n",
    "[Sample answers](https://github.com/liangz1/EchoJudge/issues/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c07bc0a4-6bb2-4e60-a5b3-e0ea56491f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a share in Delta Sharing?\n",
      "What are the considerations when choosing between Auto Loader and COPY INTO for data ingestion in Databricks?\n",
      "What kind of questions can Databricks Lakehouse Monitoring help you answer?\n",
      "What types of analysis does Databricks Lakehouse Monitoring provide?\n",
      "What is the purpose of a baseline table in Databricks Lakehouse Monitoring?\n",
      "What are the AI functions provided by Databricks for SQL users?\n",
      "How do I schedule a query execution in Azure Databricks?\n",
      "How do I register the UDAF with Spark SQL?\n"
     ]
    }
   ],
   "source": [
    "ts = traces.select(responder_name=\"directly_answered_by_gpt_4_turbo\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=-1)\n",
    "for s in ts.traces:\n",
    "    print(s.user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e7c0d-1df8-4c3a-8ed5-c3f14c3b6289",
   "metadata": {},
   "source": [
    "### Which question is answered better by GPT-4 than GPT-4-turbo?\n",
    "\n",
    "This might be an outlier, but there is one question where \"directly_answered_by_gpt_4\" is better than \"directly_answered_by_gpt_4_turbo\".\n",
    "\n",
    "- How do I implement a UserDefinedAggregateFunction in Scala for Apache Spark SQL?\n",
    "\n",
    "The correct answer should use a UserDefinedAggregateFunction with \"group by\" ([source](https://github.com/liangz1/EchoJudge/blob/456398cefc328bd507d3221925fbd39a0ba4689a/data/databricks_docs_synthetic.yaml#L6118-L6119)). Wrong answers use it as a regular UDF ([source](https://github.com/liangz1/EchoJudge/blob/456398cefc328bd507d3221925fbd39a0ba4689a/data/databricks_docs_synthetic.yaml#L6229-L6230))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fc9e2b36-fdc9-4e14-9b41-5d939d1e6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = traces.select(responder_name=\"directly_answered_by_gpt_4\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=1)\n",
    "gpt_4_rating_1 = {s.user_input for s in ts.traces}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "75248a06-bc4c-4edc-ab75-275932d8321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = traces.select(responder_name=\"directly_answered_by_gpt_4_turbo\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=0)\n",
    "gpt_4_turbo_rating_0 = {s.user_input for s in ts.traces}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2ad4a969-0731-46ec-9ec7-4fa8963a3588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'How do I implement a UserDefinedAggregateFunction in Scala for Apache Spark SQL?'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_4_rating_1.intersection(gpt_4_turbo_rating_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4b365-4b58-4f29-8de1-93975c9a576f",
   "metadata": {},
   "source": [
    "### Reusing this dataset\n",
    "\n",
    "The dataset https://github.com/liangz1/EchoJudge/blob/main/data/databricks_docs_synthetic.yaml is suitable for evaluating:\n",
    "\n",
    "- A new LLM: How well can it eliminate hallucination?\n",
    "- A new RAG: How well can it retrieve the context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed16276-1332-40e7-90f5-f9116de08ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
