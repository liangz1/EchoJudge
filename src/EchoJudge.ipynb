{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f109a2eb-272a-46c0-8a46-5a76c7bb4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "import ast\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import trace_utils\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4598ede8-7e68-491e-a93e-a12c123fcdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030bd987-fb88-4117-8420-610c5bf01802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e6972d-7dd4-4ea5-a64a-7e0f717c3448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f5a5f7-3760-4151-8aa2-617670f0c1df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9e2b36-fdc9-4e14-9b41-5d939d1e6eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75248a06-bc4c-4edc-ab75-275932d8321c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6d29302-cbb1-431f-addd-180bcdf6b620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70df314b-2a24-486f-9a05-2e345e70c0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trace(trace_id=21, user_input='how do i connect to sqlite files?', responses=[Response(responder_name='bricky@conversation_2023-11-09_04.24.44Z_master_0bcac98e_1863716079', response_text=LongStr(text='You can use the Databricks SQL Connector for Python to connect to SQLite files. You can find more information about how to use the connector in the [Databricks SQL Connector for Python documentation](https://docs.databricks.com/data/data-sources/sql-databases/sqlite.html#use-the-databricks-sql-connector-for-python).'), source=['https://learn.microsoft.com/en-us/azure/databricks/dev-tools/index-driver'], gradings=[Grading(judge_name='human', criteria=[Criterion(name='retrieval', rating=0, reason=\"we don't know what is retrieved\", tags=[]), Criterion(name='answer', rating=1, reason='helpful', tags=[])])]), Response(responder_name='gemini_pro', response_text=LongStr(text='Connecting Databricks to SQLite files involves two main approaches:\\n**1. JDBC Driver:**\\n* This is the traditional method and utilizes the SQLite JDBC driver. * Configure your Spark session with the driver and connection details to the SQLite file. * Use Spark SQL or DataFrame API to read and manipulate data in the SQLite file.\\n**Here\\'s how to do it:**\\n1. **Upload the SQLite file:** Upload your SQLite file to the Databricks File System (DBFS) using the Databricks UI or a tool like Azure Data Factory. 2. **Install the JDBC driver:** Install the SQLite JDBC driver in your Databricks cluster. You can either manually upload the driver JAR file or use the `%sql` magic command to install it dynamically. 3. **Configure your Spark session:** Set the JDBC URL, driver class, and any necessary credentials in your Spark session using `spark.read.format(\"jdbc\")`. 4. **Read data from the SQLite file:** Use Spark SQL queries or DataFrames to read and manipulate data from the SQLite file.\\n**Example code:**\\n```python # Install the JDBC driver dynamically %sql SET spark.sql.queryExecutionTimeoutSeconds = 300; %sql CREATE LIBRARY my_jdbc_lib JAR \\'/path/to/sqlite-jdbc-driver.jar\\';\\n# Define connection details url = \"jdbc:sqlite:/dbfs/<path/to/your_sqlite_file>.db\" driver = \"org.sqlite.JDBC\"\\n# Load data from the SQLite file df = spark.read.format(\"jdbc\") \\\\ .option(\"url\", url) \\\\ .option(\"driver\", driver) \\\\ .load()\\n# Analyze and manipulate the DataFrame as usual ... ```\\n**2. Delta Live Tables with Local File System (Azure Databricks only):**\\n* This method utilizes Delta Live Tables and the local file system of your Azure Databricks worker nodes. * You don\\'t need the JDBC driver, but your SQLite file must reside on the worker node\\'s local storage. * This approach offers low-latency updates and near real-time processing of changes in the SQLite file.\\n**Here\\'s how to do it:**\\n1. **Mount the local file system:** Mount the Azure Databricks worker node\\'s local file system to your DBFS using the `dbutils.fs.mount()` function. 2. **Define the Delta Live Table:** Define a Delta Live Table in your notebook, specifying the mounted location of the SQLite file. 3. **Read and manipulate data in the Delta Live Table:** Use Spark SQL queries or DataFrames to read and manipulate data in the Delta Live Table, which automatically reflects changes in the underlying SQLite file.\\n**Example code:**\\n```python # Mount the local file system dbutils.fs.mount(\"/mnt/databricks/workernode/mnt/disks/d\", \"/localmnt\")\\n# Define the Delta Live Table spark.sql(\"\"\" CREATE DELTA LIVE TABLE my_sqlite_table USING DELTA LOCATION \\'/localmnt/<path/to/your_sqlite_file>.db\\' \"\"\")\\n# Read data from the Delta Live Table df = spark.table(\"my_sqlite_table\")\\n# Analyze and manipulate the DataFrame as usual ... ```\\n**Choosing the right approach depends on your specific needs:**\\n* **JDBC Driver:** More widely applicable, works with any file system location and SQLite versions. * **Delta Live Tables:** Offers low-latency updates and near real-time processing (Azure Databricks only, file must be on worker node).\\nI hope this explanation helps you connect Databricks to your SQLite files. Feel free to ask if you have any further questions or need assistance with specific aspects of the process.'), source=['(placeholder) https://github.com/liangz1/EchoJudge/tree/main'], gradings=[Grading(judge_name='human', criteria=[Criterion(name='retrieval', rating=0, reason=\"we don't know what is retrieved\", tags=[]), Criterion(name='answer', rating=-1, reason='seems incorrect', tags=[])])]), Response(responder_name='gpt_4', response_text=LongStr(text='placeholder\\nanswer 951caf'), source=['(placeholder) https://github.com/liangz1/EchoJudge/tree/main'], gradings=[Grading(judge_name='human', criteria=[Criterion(name='retrieval', rating=0, reason=\"we don't know what is retrieved\", tags=[]), Criterion(name='answer', rating=1, reason='helpful', tags=[])])])], preferences=[Preference(judge_name='human', candidates=CandidatePair(candidate_1='bricky@conversation_2023-11-09_04.24.44Z_master_0bcac98e_1863716079', candidate_2='gemini_pro'), preferred_candidate='bricky@conversation_2023-11-09_04.24.44Z_master_0bcac98e_1863716079')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e095a70-1335-49a9-a5a6-b288d4332a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
