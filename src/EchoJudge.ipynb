{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f109a2eb-272a-46c0-8a46-5a76c7bb4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "import ast\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import trace_utils\n",
    "import yaml\n",
    "importlib.reload(trace_utils)\n",
    "from trace_utils import Trace, Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f2032e60-ecb4-4a77-83a3-e6dafe996406",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = Traces.load_from_yaml_file(\"../data/databricks_docs_synthetic.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bed31196-9e84-4888-b121-7c11ffe166bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"generated_ground_truth_answer\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6e6972d-7dd4-4ea5-a64a-7e0f717c3448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5263157894736842"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"directly_answered_by_gpt_35\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "665e51f7-3ca6-4cd8-8459-5289ae6bae27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.avg_score(responder_name=\"answered_by_gpt_35_with_ground_truth_context\", judge_name=\"liang.zhang@\", criterion=\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "722f3253-2c04-4892-ba90-ca794b0be8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the traces with -1 answers rating.\n",
    "traces.select(responder_name=\"directly_answered_by_gpt_35\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=-1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77e4d170-8db6-4f1d-8c53-a2f71ae03665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = traces.select(responder_name=\"directly_answered_by_gpt_35\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=0)\n",
    "ts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae30dd20-9cac-4694-a0e1-dbc6fc99ee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a share in Delta Sharing?\n",
      "What is Databricks-to-Databricks Delta Sharing?\n",
      "What are the considerations when choosing between Auto Loader and COPY INTO for data ingestion in Databricks?\n"
     ]
    }
   ],
   "source": [
    "for s in ts.traces:\n",
    "    print(s.user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c07bc0a4-6bb2-4e60-a5b3-e0ea56491f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = traces.select(responder_name=\"directly_answered_by_gpt_35\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=1)\n",
    "ts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da85aaa2-cf03-44c0-982a-c211f4185529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can you schedule a notebook as a task in Databricks?\n",
      "How do I implement a UserDefinedAggregateFunction in Scala for Apache Spark SQL?\n",
      "How do I register the UDAF with Spark SQL?\n"
     ]
    }
   ],
   "source": [
    "for s in ts.traces:\n",
    "    print(s.user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99680566-1ff9-4db3-80aa-43f4365dfc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.select(responder_name=\"answered_by_gpt_35_with_ground_truth_context\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=-1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf118983-846e-486c-95f0-6875f2ff17d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = traces.select(responder_name=\"answered_by_gpt_35_with_ground_truth_context\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=0)\n",
    "ts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3489944-e4a7-41e0-870d-0bb477ad7674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What types of assets can be included in a Delta Sharing share?\n",
      "How can you schedule a notebook as a task in Databricks?\n",
      "What are the AI functions provided by Databricks for SQL users?\n"
     ]
    }
   ],
   "source": [
    "for s in ts.traces:\n",
    "    print(s.user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99f5a5f7-3760-4151-8aa2-617670f0c1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces.select(responder_name=\"generated_ground_truth_answer\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=-1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fc9e2b36-fdc9-4e14-9b41-5d939d1e6eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = traces.select(responder_name=\"generated_ground_truth_answer\", judge_name=\"liang.zhang@\", criterion=\"answer\", rating=0)\n",
    "ts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32827efe-6c9a-4308-a4da-3f72cb2c772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can you schedule a notebook as a task in Databricks?\n",
      "How do I implement a UserDefinedAggregateFunction in Scala for Apache Spark SQL?\n"
     ]
    }
   ],
   "source": [
    "for s in ts.traces:\n",
    "    print(s.user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75248a06-bc4c-4edc-ab75-275932d8321c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the traces with a certain preference.\n",
    "ts = traces.select(judge_name=\"liang.zhang@\", preferred_candidate=\"directly_answered_by_gpt_35\")\n",
    "ts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6d29302-cbb1-431f-addd-180bcdf6b620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can you schedule a notebook as a task in Databricks?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.traces[0].user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4a969-0731-46ec-9ec7-4fa8963a3588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
